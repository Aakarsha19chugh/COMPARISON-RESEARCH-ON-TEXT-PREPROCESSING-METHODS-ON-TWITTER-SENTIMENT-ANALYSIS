# -*- coding: utf-8 -*-
"""twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XCwKIhanCAnF8-dD0Czs1Rlxd_RZtacP
"""

import re

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from io import StringIO

from google.colab import files
uploaded = files.upload()

print (uploaded['twitter4242.txt'][:200].decode('utf-8') + '...')

import io
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
data_path = 'twitter4242.txt'
with open(data_path, 'rb') as f:
    lines = f.read()

lines

from functools import partial
from collections import Counter
import nltk
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer

def removeUnicode(text):
    """ Removes unicode strings like "\u002c" and "x96" """
    text = re.sub(r'(\\u[0-9A-Fa-f]+)',r'', text)       
    text = re.sub(r'[^\x00-\x7f]',r'',text)
    return text

def replaceURL(text):
    """ Replaces url address with "url" """
    text = re.sub('((www\.[^\s]+)|(https?://[^\s]+))','url',text)
    text = re.sub(r'#([^\s]+)', r'\1', text)
    return text
  
def replaceAtUser(text):
    """ Replaces "@user" with "atUser" """
    text = re.sub('@[^\s]+','atUser',text)
    return text

def removeHashtagInFrontOfWord(text):
    """ Removes hastag in front of a word """
    text = re.sub(r'#([^\s]+)', r'\1', text)
    return text
  
def removeNumbers(text):
    """ Removes integers """
    text = ''.join([i for i in text if not i.isdigit()])         
    return text

def replaceMultiExclamationMark(text):
    """ Replaces repetitions of exlamation marks """
    text = re.sub(r"(\!)\1+", ' multiExclamation ', text)
    return text

def replaceMultiQuestionMark(text):
    """ Replaces repetitions of question marks """
    text = re.sub(r"(\?)\1+", ' multiQuestion ', text)
    return text
  
def replaceMultiStopMark(text):
    """ Replaces repetitions of stop marks """
    text = re.sub(r"(\.)\1+", ' multiStop ', text)
    return text

uploadedslang = files.upload()

""" Creates a dictionary with slangs and their equivalents and replaces them """
with open('slang.txt') as file:
    slang_map = dict(map(str.strip, line.partition('\t')[::2])
    for line in file if line.strip())

slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex
regex = re.compile(r"\b({})\b".format("|".join(map(re.escape, slang_words))))
replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])

""" Replaces contractions from a string to their equivalents """
contraction_patterns = [ (r'won\'t', 'will not'), (r'can\'t', 'cannot'), (r'i\'m', 'i am'), (r'ain\'t', 'is not'), (r'(\w+)\'ll', '\g<1> will'), (r'(\w+)n\'t', '\g<1> not'),
                         (r'(\w+)\'ve', '\g<1> have'), (r'(\w+)\'s', '\g<1> is'), (r'(\w+)\'re', '\g<1> are'), (r'(\w+)\'d', '\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]

def replaceContraction(text):
    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]
    for (pattern, repl) in patterns:
        (text, count) = re.subn(pattern, repl, text)
    return text

def replaceElongated(word):
    """ Replaces an elongated word with its basic form, unless the word exists in the lexicon """

    repeat_regexp = re.compile(r'(\w*)(\w)\2(\w*)')
    repl = r'\1\2\3'
    if wordnet.synsets(word):
        return word
    repl_word = repeat_regexp.sub(repl, word)
    if repl_word != word:      
        return replaceElongated(repl_word)
    else:       
        return repl_word

def removeEmoticons(text):
    """ Removes emoticons from text """
    text = re.sub(':\)|;\)|:-\)|\(-:|:-D|=D|:P|xD|X-p|\^\^|:-*|\^\.\^|\^\-\^|\^\_\^|\,-\)|\)-:|:\'\(|:\(|:-\(|:\S|T\.T|\.\_\.|:<|:-\S|:-<|\*\-\*|:O|=O|=\-O|O\.o|XO|O\_O|:-\@|=/|:/|X\-\(|>\.<|>=\(|D:', '', text)
    return text

"""**SPELL CORRECTION**"""

uploadedspell = files.upload()

""" Spell Correction http://norvig.com/spell-correct.html """
def words(text): return re.findall(r'\w+', text.lower())

WORDS = Counter(words(open('corporaForSpellCorrection.txt').read()))

def P(word, N=sum(WORDS.values())): 
    """P robability of `word`. """
    return WORDS[word] / N
  
def spellCorrection(word): 
    """ Most probable spelling correction for word. """
    return max(candidates(word), key=P)
  
def candidates(word): 
    """ Generate possible spelling corrections for word. """
    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])
  
def known(words): 
    """ The subset of `words` that appear in the dictionary of WORDS. """
    return set(w for w in words if w in WORDS)
  
def edits1(word):
    """ All edits that are one edit away from `word`. """
    letters    = 'abcdefghijklmnopqrstuvwxyz'
    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]
    deletes    = [L + R[1:]               for L, R in splits if R]
    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]
    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]
    inserts    = [L + c + R               for L, R in splits for c in letters]
    return set(deletes + transposes + replaces + inserts)
  
def edits2(word): 
    """ All edits that are two edits away from `word`. """
    return (e2 for e1 in edits1(word) for e2 in edits1(e1))

"""**Replacing  Negations **"""

def replace(word, pos=None):
    """ Creates a set of all antonyms for the word and if there is only one antonym, it returns it """
    antonyms = set()
    for syn in wordnet.synsets(word, pos=pos):
      for lemma in syn.lemmas():
        for antonym in lemma.antonyms():
          antonyms.add(antonym.name())
    if len(antonyms) == 1:
      return antonyms.pop()
    else:
      return None

def replaceNegations(text):
    """ Finds "not" and antonym for the next word and if found, replaces not and the next word with the antonym """
    i, l = 0, len(text)
    words = []
    while i < l:
      word = text[i]
      if word == 'not' and i+1 < l:
        ant = replace(text[i+1])
        if ant:
          words.append(ant)
          i += 2
          continue
      words.append(word)
      i += 1
    return str(words)

"""**Replacing Negations Done **"""

def addNotTag(text):
	""" Finds "not,never,no" and adds the tag NEG_ to all words that follow until the next punctuation """
	transformed = re.sub(r'\b(?:not|never|no)\b[\w\s]+[^\w\s]', 
       lambda match: re.sub(r'(\s+)(\w+)', r'\1NEG_\2', match.group(0)), 
       text,
       flags=re.IGNORECASE)
	return transformed

def addCapTag(word):
    """ Finds "not,never,no" and adds the tag ALL_CAPS_ to all words that follow until the next punctuation """
    if(len(re.findall("[A-Z]{3,}", word))):
        word = word.replace('\\', '' )
        transformed = re.sub("[A-Z]{3,}", "ALL_CAPS_"+word, word)
        return transformed
    else:
        return word

"""# Implementation -------------------------------------"""

from time import time
import numpy as np
import string

"""### Tokenizes a text to its words, removes and replaces some of them"""

import nltk
nltk.download('stopwords')
finalTokens = [] # all tokens
stoplist = stopwords.words('english')
my_stopwords = "multiexclamation multiquestion multistop url atuser st rd nd th am pm"        #Extra stop words

stoplist = stoplist + my_stopwords.split()
allowedWordTypes = ["J","R","V","N"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging

stemmer = PorterStemmer() # set stemmer

t0 = time()
totalSentences = 0
totalEmoticons = 0
totalSlangs = 0
totalSlangsFound = []
totalElongated = 0
totalMultiExclamationMarks = 0
totalMultiQuestionMarks = 0
totalMultiStopMarks = 0
totalAllCaps = 0

lines=lines.split(b'\n')

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

output0 = " "
output1 = " "
output2 = " "
output3 = " "
output4 = " "
output5 = " "
output6 = " "
output7 = " "

for line in lines:
    totalSentences += 1
    feat = []
    columns = line.split(b'\t')
    columns = [col.strip() for col in columns]
    ##print(columns)
    textID = str(columns[0])
    #columns[2].strip()
    y = columns[2].decode('utf8','ignore')
    # print(y)
    text=columns[1].strip()
    
    text = removeUnicode(columns[1].decode('utf8','ignore'))              # Technique 0-------------------
    
    output0 = output0 + text+"\n"
    
    text = replaceURL(text) # Technique 1
    text = replaceAtUser(text) # Technique 1
    text = removeHashtagInFrontOfWord(text) # Technique 1
    
    output1 = output1+ text+"\n"
    
    text = replaceSlang(text) # Technique 2: replaces slang words and abbreviations with their equivalents
    output2 = output2+ text+"\n"
    
    text = replaceContraction(text) # Technique 3: replaces contractions to their equivalents
    output3 = output3 + text+"\n"
    
    text = removeNumbers(text) # Technique 4: remove integers from text
    output4 = output4 + text+"\n"
    
    text = removeEmoticons(text) # removes emoticons from text
    output5 = output5 + text+"\n"
    
    
    text = replaceMultiExclamationMark(text) # Technique 6: replaces repetitions of exlamation marks with the tag "multiExclamation"        #Technique 6
    text = replaceMultiQuestionMark(text) # Technique 6: replaces repetitions of question marks with the tag "multiQuestion"
    text = replaceMultiStopMark(text) # Technique 6: replaces repetitions of stop marks with the tag "multiStop"
   # text = replaceNegations(text)  # replace negations
    text= replaceElongated(text)
    output6 = output6+ "\n" + text

"""## Remove all unicode characters"""

output0

"""## Replaced all the URLs , Removed @ and removed # in front of sentences"""

output1

"""## Replace all the slang words"""

output2

"""## Replace contractions (e.g - there's changed to there is)"""

output3

"""## Removed Integers"""

output4

"""## Remove emoticons"""

output5

"""## Replace multistop words, multi question marks and multi exclamation marks with tags, negations and elongated"""

output6

clean_text = " "
clean_text = clean_text + output6

clean_list = clean_text.split("\n")

len(clean_list)

Y = " "
for line in lines:
    columns = line.split(b'\t')
    columns = [col.strip() for col in columns]
    y = columns[2].decode('utf8','ignore')
    Y = Y + "\n" + y

Y_list = Y.split("\n")

len(Y_list)

preprocessed_dataset1 = pd.DataFrame(
    {'Tweets': clean_list,
     'polarity': Y_list
    })

from google.colab import files

preprocessed_dataset1.to_csv('twitter4242_preprocessed.csv')
files.download('twitter4242_preprocessed.csv')

"""# SECOND DATASET  ( STS- GOLD)"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import files
uploadedsts = files.upload()

import io
df1 = pd.read_csv(io.StringIO(uploadedsts['sts_gold.csv'].decode('utf-8')))

df1.head(3)

uploadresult_sts =  files.upload()

import time
t01 = time.time()
totalSentences1 = 0
totalEmoticons1 = 0
totalSlangs1 = 0
totalSlangsFound1 = []
totalElongated1 = 0
totalMultiExclamationMarks1 = 0
totalMultiQuestionMarks1 = 0
totalMultiStopMarks1 = 0
totalAllCaps1 = 0

output11 = " " 
output22 = " "
output33 = " "
output44 = " "
output55 = " "
output66 = " "
str1 = " "

X = df1.iloc[:,2]
textID1 = df1.iloc[:,0]
y1 = df1.iloc[:,1]
for i in X:
  totalSentences1 += 1
  feat1 = []
  columnsnew =i.split(r'\t')
  columnsnew = [colnew.strip() for colnew in columnsnew]
  #print(columns)
  
  str1=' '.join(columnsnew)

  str1  = replaceURL(str1) # Technique 1
  str1  = replaceAtUser( str1 ) # Technique 1
  str1  = removeHashtagInFrontOfWord( str1 ) # Technique 1
  output11 = output11 +  str1+"\n"
  
  
  str1 = replaceSlang(str1) # Technique 2: replaces slang words and abbreviations with their equivalents
  output22 = output22+ str1 +"\n"
    
  str1 = replaceContraction(str1) # Technique 3: replaces contractions to their equivalents
  output33 = output33 + str1 +"\n"
    
  str1 = removeNumbers(str1) # Technique 4: remove integers from text
  output44 = output44 + str1 +"\n"
    
  str1 = removeEmoticons(str1) # removes emoticons from text
  output55 = output55 + str1 +"\n"
  
  str1 = replaceMultiExclamationMark(str1) # Technique 6: replaces repetitions of exlamation marks with the tag "multiExclamation"        #Technique 6
  str1 = replaceMultiQuestionMark(str1) # Technique 6: replaces repetitions of question marks with the tag "multiQuestion"
  str1 = replaceMultiStopMark(str1) # Technique 6: replaces repetitions of stop marks with the tag "multiStop"
  
  output66 = output66 + "\n" +str1

"""## Replaced all the URLs , Removed @ and removed # in front of sentences"""

output11

"""## Replace all slang words"""

output22

"""## Replace contractions"""

output33

"""## Remove integers"""

output44

"""## Remove emoticons"""

output55

"""## Replace multistop words, multi question marks and multi exclamation marks with tags"""

output66

clean_text2 = " "
clean_text2 = clean_text2 + output66

clean_list2 = clean_text2.split("\n")

len(clean_list2)

clean_list2.remove(clean_list2[0])

Y2 = df1.iloc[:,1]

len(Y2)

len(clean_list2)

preprocessed_dataset2= pd.DataFrame(
    {'Tweets': clean_list2,
     'polarity': Y2
    })

preprocessed_dataset2.head()

from google.colab import files

preprocessed_dataset2.to_csv('STSGOLD_preprocessed.csv')
files.download('STSGOLD_preprocessed.csv')

"""# 3rd dataset Semcal A"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import files
uploadedsem1 = files.upload()

import io
for fn in uploadedsem1.keys():
  print('User uploaded file "{name1}" with length {length} bytes'.format(
      name1=fn, length=len(uploadedsem1[fn])))
data_path = 'semval2017.txt'
with open(data_path, 'rb') as f:
    lines3 = f.read()

lines3

lines3=lines3.split(b'\n')

output03 = " "
output13 = " "
output23 = " "
output33 = " "
output43 = " "
output53 = " "
output63 = " "
totalSentences3 =0



for line in lines3:
    totalSentences3 += 1
    feat3 = []
    columns3 = line.split(b'\t')
    columns3 = [colnew.strip() for colnew in columns3]
    ##print(columns)
    textID3 = str(columns3[0])
    #columns[2].strip()
    y3 = columns3[1].decode('utf8','ignore')
    # print(y)
    text3=columns3[2].strip()
    
    text3 = removeUnicode(columns3[2].decode('utf8','ignore'))              # Technique 0-------------------
    
    output03 = output03 + text3
    
    text3 = replaceURL(text3) # Technique 1
    text3 = replaceAtUser(text3) # Technique 1
    text3 = removeHashtagInFrontOfWord(text3) # Technique 1
    
    output13 = output13+ text3
    
    text3 = replaceSlang(text3) # Technique 2: replaces slang words and abbreviations with their equivalents
    output23 = output23+ text3
    
    text3 = replaceContraction(text3) # Technique 3: replaces contractions to their equivalents
    output33 = output33 + text3
    
    text3 = removeNumbers(text3) # Technique 4: remove integers from text
    output43= output43 + text3
    
    text3 = removeEmoticons(text3) # removes emoticons from text
    output53 = output53 + text3

    text3 = replaceMultiExclamationMark(text3) # Technique 6: replaces repetitions of exlamation marks with the tag "multiExclamation"        #Technique 6
    text3 = replaceMultiQuestionMark(text3) # Technique 6: replaces repetitions of question marks with the tag "multiQuestion"
    text3 = replaceMultiStopMark(text3) # Technique 6: replaces repetitions of stop marks with the tag "multiStop"
    text = replaceNegations(text3)  # replace negations
    text= replaceElongated(text3)
    output63 = output63+ "\n" + text3

"""**Remove all unicode characters**"""

output03

"""**Replaced all the URLs , Removed @ and removed # in front of sentences**"""

print(output13)

"""Replace all the slang words"""

print(output23)

"""**Replace contractions (e.g - there's changed to there is)**"""

print(output33)

"""**Removed Integers**"""

print(output43)

"""**Remove emoticons**"""

print(output53)

"""**Replace multistop words, multi question marks and multi exclamation marks with tags**"""

output63

clean_text3 = " "
clean_text3 = clean_text3 + output63
clean_list3 = clean_text3.split("\n")



len(clean_list3)

Y3 = " "
for line in lines3:
    columns = line.split(b'\t')
    columns = [col.strip() for col in columns]
    y3 = columns[1].decode('utf8','ignore')
    Y3 = Y3 + "\n" + y3

Y3_list = Y3.split("\n")

len(Y3_list)

preprocessed_dataset3 = pd.DataFrame(
    {'Tweets': clean_list3,
     'polarity': Y3_list
    })

preprocessed_dataset3.head()

from google.colab import files

preprocessed_dataset3.to_csv('semeval2017_preprocessed.csv')
files.download('semeval2017_preprocessed.csv')

"""## Upload Preprocessed Tweets -DATASET 1 - TWITTER4242"""

from google.colab import files
uploadedpre1 = files.upload()

import pandas as pd
import io
df_twitter4242 = pd.read_csv(io.StringIO(uploadedpre1['twitter4242_preprocessed.csv'].decode('utf-8')))

"""## Tokenization"""

tokenized_tweet = df_twitter4242['Tweets'].apply(lambda x: x.split())
tokenized_tweet.head()

len(df_twitter4242['Tweets'])

"""## Remove Stopwords

## Stemming
"""

from nltk.stem.porter import *
stemmer = PorterStemmer()
tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming
tokenized_tweet.head()



for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])

df_twitter4242['Tweets'] = tokenized_tweet

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

corpus = []
for i in range(0,2000):
  tweet = re.sub('[^a-zA-Z]', ' ', df_twitter4242['Tweets'][i])
  tweet = tweet.lower()
  tweet = tweet.split()
  review = [word for word in tweet if not word in set(stopwords.words('english'))]
  tweet = ' '.join(tweet)
  corpus.append(tweet)

tokenized_tweet = df_twitter4242['Tweets'].apply(lambda x: x.split())
tokenized_tweet.head()

df_twitter4242['Tweets']=corpus

"""## Visualization"""

!pip install wordcloud

import matplotlib.pyplot as plt 
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
all_words = ' '.join([text for text in df_twitter4242['Tweets'] ])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()





"""## Dataset 1 - Twitter4242

## Split into train and test
"""

from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_twitter4242['Tweets'], df_twitter4242['polarity'], test_size = 0.20, random_state = 3)

"""## Feature Extraction"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(stop_words='english')
transformer = TfidfTransformer(norm='l2',sublinear_tf=True)
## for transforming the 80% of the train data ##
X_train_counts = count_vect.fit_transform(X_train)
X_train_tfidf = transformer.fit_transform(X_train_counts)
## for transforming the 20% of the train data which is being used for testing ##
x_test_counts = count_vect.transform(X_test)
x_test_tfidf = transformer.transform(x_test_counts)

"""## Random forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
model1 = RandomForestClassifier(n_estimators=200,criterion = 'gini')
model1.fit(X_train_tfidf,y_train)

predictions = model1.predict(x_test_tfidf)

cal_accuracy(y_test,predictions)

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
classifier2 = LogisticRegression(random_state=0)
classifier2.fit(X_train_tfidf,y_train)

y_pred2 = classifier2.predict(x_test_tfidf)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Function to calculate accuracy
def cal_accuracy(y_test, y_pred2):
     
    print("Confusion Matrix: ",
    confusion_matrix(y_test, y_pred2))
     
    print ("Accuracy : ",
    accuracy_score(y_test,y_pred2)*100)
     
    print("Report : ",
    classification_report(y_test, y_pred2))

cal_accuracy(y_test,y_pred2)

"""## KNN"""

from sklearn.neighbors import KNeighborsClassifier
classifier1 = KNeighborsClassifier()
classifier1 = classifier1.fit(X_train_tfidf,y_train)

y_pred1 = classifier1.predict(x_test_tfidf)

cal_accuracy(y_test,y_pred1)

"""## SVM Classifier"""

from sklearn.svm import SVC
clf = SVC(kernel = "linear")
clf.fit(X_train_tfidf,y_train)
pred = clf.predict(x_test_tfidf)

cal_accuracy(y_test,pred)

"""## Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier
gbm0 = GradientBoostingClassifier(random_state=100)
gbm0 = gbm0.fit(X_train_tfidf,y_train)
z1 = gbm0.predict(x_test_tfidf)
cal_accuracy(y_test,z1)



"""VOTING CLASSIFIER"""



rf = RandomForestClassifier(n_estimators=150)
lr = LogisticRegression(random_state=0)
sv = SVC(kernel = "linear")

clf_labels = ['RandomForestClassifier', 'LogisticRegression', 'SVC']

from sklearn.model_selection import cross_val_score
for clf , label in zip([rf , lr , sv], clf_labels):
    scores = cross_val_score(estimator = clf , X= X_train_tfidf , y= y_train , cv=10 , scoring = 'accuracy')
    print('Accuracy : %0.2f (+/- %0.2f) [%s]' % (scores.mean(), scores.std(), label))

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean= True)
from sklearn.ensemble import VotingClassifier
from sklearn.pipeline import Pipeline


pipe_lr = Pipeline([['lr',lr]])
pipe_sv= Pipeline([['sv',sv]])

mv = VotingClassifier(estimators = [('lr',pipe_lr) , ('sv',pipe_sv) , ('rf',rf)], voting = "hard")

mv = mv.fit(X_train_tfidf,y_train)

z4 = mv.predict(x_test_tfidf)
cal_accuracy(y_test,z1)

from sklearn.ensemble import BaggingClassifier
from sklearn import tree
model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))
model.fit(X_train_tfidf,y_train)
pred2 = model.predict(x_test_tfidf)
cal_accuracy(y_test,pred2)

!pip install mlxtend

"""## Logistic

## Upload Preprocessed Tweets -DATASET 2 - STS gold**
"""

from google.colab import files
uploadedpre2 = files.upload()

import io
df_stsgold = pd.read_csv(io.StringIO(uploadedpre2['STSGOLD_preprocessed.csv'].decode('utf-8')))

df_stsgold .head(5)

"""**TOKENIZATION**"""

tokenized_tweet2 = df_stsgold['Tweets'].apply(lambda x: x.split())
tokenized_tweet2.head()

"""**STEMMING**"""

from nltk.stem.porter import *
stemmer = PorterStemmer()

tokenized_tweet2 = tokenized_tweet2.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming
tokenized_tweet2.head()

for i in range(len(tokenized_tweet2)):
    tokenized_tweet2[i] = ' '.join(tokenized_tweet2[i])

df_stsgold['Tweets'] = tokenized_tweet2

"""**VISUALIZATION**"""

!pip install wordcloud

import matplotlib.pyplot as plt 
import seaborn as sns
from wordcloud import WordCloud
all_words = ' '.join([text for text in df_stsgold_2['Tweets'] ])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""**DATASET 2 - STS_GOLD **"""

from sklearn.cross_validation import train_test_split

X2_train, X2_test, y2_train, y2_test = train_test_split(df_stsgold['Tweets'], df_stsgold['polarity'], test_size = 0.20, random_state = 3)

"""**FEATURE EXTRACTION**"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(stop_words='english')
transformer = TfidfTransformer(norm='l2',sublinear_tf=True)
## for transforming the 80% of the train data ##
X2_train_counts = count_vect.fit_transform(X2_train)
X2_train_tfidf = transformer.fit_transform(X2_train_counts)
## for transforming the 20% of the train data which is being used for testing ##
x2_test_counts = count_vect.transform(X2_test)
x2_test_tfidf = transformer.transform(x2_test_counts)

"""## RANDOM FOREST CLASSIFIER"""

from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier(n_estimators=150)
model2.fit(X2_train_tfidf,y2_train)

predictions2 = model2.predict(x2_test_tfidf)

cal_accuracy(y2_test,predictions2)

"""## LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
classifier22 = LogisticRegression(random_state=0)
classifier22.fit(X2_train_tfidf,y2_train)

y_pred22 = classifier22.predict(x2_test_tfidf)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Function to calculate accuracy
def cal_accuracy(y2_test, y_pred22):
     
    print("Confusion Matrix: ",
        confusion_matrix(y2_test, y_pred22))
     
    print ("Accuracy : ",
    accuracy_score(y2_test,y_pred22)*100)
     
    print("Report : ",
    classification_report(y2_test, y_pred22))

cal_accuracy(y2_test,y_pred22)

"""## K Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier
classifier12 = KNeighborsClassifier()
classifier12 = classifier12.fit(X2_train_tfidf,y2_train)

y_pred12 = classifier12.predict(x2_test_tfidf)

cal_accuracy(y2_test,y_pred12)

"""## SVM"""

from sklearn.svm import SVC
clf2 = SVC(kernel = "linear")
clf2.fit(X2_train_tfidf,y2_train)
pred2 = clf2.predict(x2_test_tfidf)

cal_accuracy(y2_test,pred2)

"""## Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier
gbm02 = GradientBoostingClassifier(random_state=10)
gbm02 = gbm02.fit(X2_train_tfidf,y2_train)
z2 = gbm02.predict(x2_test_tfidf)
cal_accuracy(y2_test,z2)

"""## Bagging Classifier"""

from sklearn.ensemble import BaggingClassifier
from sklearn import tree
model22 = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))
model22.fit(X2_train_tfidf,y2_train)
z5 = model22.predict(x2_test_tfidf)
cal_accuracy(y2_test,z5)

"""## Voting"""

rf = RandomForestClassifier(n_estimators=150)
lr = LogisticRegression(random_state=0)
sv = SVC(kernel = "linear")

clf_labels = ['RandomForestClassifier', 'LogisticRegression', 'SVC']

from sklearn.model_selection import cross_val_score
for clf , label in zip([rf , lr , sv], clf_labels):
    scores = cross_val_score(estimator = clf , X= X2_train_tfidf , y= y2_train , cv=10 , scoring = 'accuracy')
    print('Accuracy : %0.2f (+/- %0.2f) [%s]' % (scores.mean(), scores.std(), label))

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean= True)
from sklearn.ensemble import VotingClassifier
from sklearn.pipeline import Pipeline


pipe_lr = Pipeline([['lr',lr]])
pipe_sv= Pipeline([['sv',sv]])

mv = VotingClassifier(estimators = [('lr',pipe_lr) , ('sv',pipe_sv) , ('rf',rf)], voting = "hard")

mv = mv.fit(X2_train_tfidf,y2_train)

z4 = mv.predict(x2_test_tfidf)
cal_accuracy(y2_test,z4)

"""# **Dataset 3- Semeval2017**

# Upload dataset
"""

from google.colab import files
uploadedpre1 = files.upload()

print (uploadedpre1['semeval2017_preprocessed.csv'][:200].decode('utf-8') + '...')

import io
df_semeval2017 = pd.read_csv(io.StringIO(uploadedpre1['semeval2017_preprocessed.csv'].decode('utf-8')))

"""# Tokenization"""

tokenized_tweet3 = df_semeval2017['Tweets'].apply(lambda x: x.split() if type(x) is str else 'empty')
tokenized_tweet3.head()

"""# Stemming"""

from nltk.stem.porter import *
stemmer = PorterStemmer()

tokenized_tweet3 = tokenized_tweet3.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming
tokenized_tweet3.head()

for i in range(len(tokenized_tweet3)):
    tokenized_tweet3[i] = ' '.join(tokenized_tweet3[i])

df_semeval2017['Tweets'] = tokenized_tweet3

"""# Visualization"""

import matplotlib.pyplot as plt 
import seaborn as sns
from wordcloud import WordCloud
all_words = ' '.join([text for text in df_semeval2017['Tweets'] ])
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

"""# Splitting into train and set sets"""

from sklearn.cross_validation import train_test_split
X3_train, X3_test, y3_train, y3_test = train_test_split(df_semeval2017['Tweets'], df_semeval2017['polarity'], test_size = 0.20, random_state = 3)

"""# Feature Extraction"""

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(stop_words='english')
transformer = TfidfTransformer(norm='l2',sublinear_tf=True)
## for transforming the 80% of the train data ##
X3_train_counts = count_vect.fit_transform(X3_train)
X3_train_tfidf = transformer.fit_transform(X3_train_counts)
## for transforming the 20% of the train data which is being used for testing ##
x3_test_counts = count_vect.transform(X3_test)
x3_test_tfidf = transformer.transform(x3_test_counts)

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=300)
model.fit(X3_train_tfidf,y3_train)

predictions33 = model.predict(x3_test_tfidf)

from sklearn.metrics import accuracy_score
print(accuracy_score(y3_test,predictions))

cal_accuracy(y3_test,predictions3)

"""# Logistic Regression"""

from sklearn.linear_model import LogisticRegression
classifier3 = LogisticRegression(random_state=0)
classifier3.fit(X3_train_tfidf,y3_train)

y_pred3 = classifier3.predict(x3_test_tfidf)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

cal_accuracy(y3_test,y_pred3)

"""# K-nearest neighbours"""

from sklearn.neighbors import KNeighborsClassifier
classifier2 = KNeighborsClassifier()
classifier2 = classifier2.fit(X_train_tfidf,y3_train)

y_pred2 = classifier2.predict(x_test_tfidf)

cal_accuracy(y3_test,y_pred2)

"""##SVM Classifier"""

from sklearn.svm import SVC
clf = SVC(kernel = 'linear')
clf.fit(X_train_tfidf,y3_train)
pred = clf.predict(x_test_tfidf)

cal_accuracy(y3_test,pred)

rf = RandomForestClassifier(n_estimators=150)
lr = LogisticRegression(random_state=0)
sv = SVC(kernel = "linear")

clf_labels = ['RandomForestClassifier', 'LogisticRegression', 'SVC']

from sklearn.model_selection import cross_val_score
for clf , label in zip([rf , lr , sv], clf_labels):
    scores = cross_val_score(estimator = clf , X= X3_train_tfidf , y= y3_train , cv=10 , scoring = 'accuracy')
    print('Accuracy : %0.2f (+/- %0.2f) [%s]' % (scores.mean(), scores.std(), label))

from sklearn.preprocessing import StandardScaler
sc = StandardScaler(with_mean= True)
from sklearn.ensemble import VotingClassifier
from sklearn.pipeline import Pipeline


pipe_lr = Pipeline([['lr',lr]])
pipe_sv= Pipeline([['sv',sv]])

mv = VotingClassifier(estimators = [('lr',pipe_lr) , ('sv',pipe_sv) , ('rf',rf)], voting = "hard")

mv = mv.fit(X_train_tfidf,y_train)

z4 = mv.predict(x_test_tfidf)
cal_accuracy(y_test,z1)

"""## Bagging classifier"""

from sklearn.ensemble import BaggingClassifier
from sklearn import tree
model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))
model.fit(X3_train_tfidf,y3_train)
pred2 = model.predict(x3_test_tfidf)
cal_accuracy(y3_test,pred2)

from sklearn.ensemble import GradientBoostingClassifier
gbm0 = GradientBoostingClassifier(random_state=100)
gbm0 = gbm0.fit(X3_train_tfidf,y3_train)
z1 = gbm0.predict(x3_test_tfidf)
cal_accuracy(y3_test,z1)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import files
uploadedbook = files.upload()

print (uploadedbook['4thdataset.csv'][:200].decode('utf-8') + '...')

uploadedbook

import io
df4 = pd.read_csv(io.StringIO(uploadedbook['4thdataset.csv'].decode('utf-8')))

df4



print("Starting preprocess..\n")

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
finalTokens = [] # all tokens
stoplist = stopwords.words('english')
my_stopwords = "multiexclamation multiquestion multistop url atuser st rd nd th am pm"        #Extra stop words

from nltk.stem.wordnet import WordNetLemmatizer
from nltk.stem import PorterStemmer
stoplist = stoplist + my_stopwords.split()
allowedWordTypes = ["J","R","V","N"] #  J is Adject, R is Adverb, V is Verb, N is Noun. These are used for POS Tagging
lemmatizer = WordNetLemmatizer() # set lemmatizer
stemmer = PorterStemmer() # set stemmer





import time
t01 = time.time()
totalSentences1 = 0
totalEmoticons1 = 0
totalSlangs1 = 0
totalSlangsFound1 = []
totalElongated1 = 0
totalMultiExclamationMarks1 = 0
totalMultiQuestionMarks1 = 0
totalMultiStopMarks1 = 0
totalAllCaps1 = 0

output11 = " " 
output22 = " "
output33 = " "
output44 = " "
output55 = " "
output66 = " "
str4 = " "

nltk.download('punkt')

nltk.download('averaged_perceptron_tagger')

nltk.download('wordnet')

X = df4.iloc[:,0]
y1 = df4.iloc[:,1]



X = df4.iloc[:,0]
y1 = df4.iloc[:,1]
for i in X:
  totalSentences1 += 1
  feat1 = []
  columnsnew =i.split(r'\t')
  columnsnew = [colnew.strip() for colnew in columnsnew]
  
  wordCountBefore1 = len(re.findall(r'\w+', str4)) # word count of one sentence before preprocess
  
  str4=' '.join(columnsnew)
  wordCountBefore1 = len(re.findall(r'\w+', str4)) # word count of one sentence before preprocess
  str4  = replaceURL(str4) # Technique 1
  str4  = replaceAtUser( str4 ) # Technique 1
  str4  = removeHashtagInFrontOfWord( str4) # Technique 1
  output11 = output11 +  str4+"\n"
 

  temp_slangs1, temp_slangsFound1 = countSlang(str4)
  totalSlangs1 += temp_slangs1 # total slangs for all sentences
  for word in temp_slangsFound1:
        totalSlangsFound1.append(str4) # all the slangs found in all sentences
    # print( len(totalSlangsFound))
    
  str4 = replaceSlang(str4) # Technique 2: replaces slang words and abbreviations with their equivalents
  output22 = output22+ str4 +"\n"
    
  str4 = replaceContraction(str4) # Technique 3: replaces contractions to their equivalents
  output33 = output33 + str4 +"\n"
    
  str4 = removeNumbers(str4) # Technique 4: remove integers from text
  output44 = output44 + str4 +"\n"
    
  emoticons1 = countEmoticons(str4) # Technique 5: how many emoticons in this sentence
  totalEmoticons1 += emoticons1
    
  str4 = removeEmoticons(str4) # removes emoticons from text
  output55 = output55 + str4 +"\n"
  
  
  totalAllCaps1 += countAllCaps(str4)

  totalMultiExclamationMarks1 += countMultiExclamationMarks(str4) # how many repetitions of exlamation marks in this sentence
  totalMultiQuestionMarks1 += countMultiQuestionMarks(str4) # how many repetitions of question marks in this sentence
  totalMultiStopMarks1 += countMultiStopMarks(str4) # how many repetitions of stop marks in this sentence
    
  str4 = replaceMultiExclamationMark(str4) # Technique 6: replaces repetitions of exlamation marks with the tag "multiExclamation"        #Technique 6
  str4 = replaceMultiQuestionMark(str4) # Technique 6: replaces repetitions of question marks with the tag "multiQuestion"
  str4 = replaceMultiStopMark(str4) # Technique 6: replaces repetitions of stop marks with the tag "multiStop"
  output66 = output66 +str4 +"\n"
    
  totalElongated1 += countElongated(str4) # how many elongated words emoticons in this sentence



output66

clean_text4 = " "
clean_text4 = clean_text4 + output66

clean_list4 = output66.split('\n')

clean_list4.remove(clean_list4[0])

len(clean_list4)

Y2 = df4.iloc[:,1]

preprocessed_dataset4= pd.DataFrame(
    {'Tweets': clean_list4,
     'polarity': Y2
    })

preprocessed_dataset4.head()

from google.colab import files

preprocessed_dataset4.to_csv('dataset4_preprocessed.csv')
files.download('dataset4_preprocessed.csv')

from google.colab import files
uploadedpre4 = files.upload()

print (uploadedpre4['dataset4_preprocessed.csv'][:200].decode('utf-8') + '...')

import io
df_4dataset = pd.read_csv(io.StringIO(uploadedpre4['dataset4_preprocessed.csv'].decode('utf-8')))

df_4dataset

from sklearn.cross_validation import train_test_split

X4_train, X4_test, y4_train, y4_test = train_test_split(df_4dataset['Tweets'], df_4dataset['polarity'], test_size = 0.20, random_state = 3)

from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer(stop_words='english')
transformer = TfidfTransformer(norm='l2',sublinear_tf=True)
## for transforming the 80% of the train data ##
X4_train_counts = count_vect.fit_transform(X4_train)
X4_train_tfidf = transformer.fit_transform(X4_train_counts)
## for transforming the 20% of the train data which is being used for testing ##
x4_test_counts = count_vect.transform(X4_test.astype('U'))
x4_test_tfidf = transformer.transform(x4_test_counts.astype('U'))

from sklearn.ensemble import RandomForestClassifier
model2 = RandomForestClassifier(n_estimators=150)
model2.fit(X4_train_tfidf,y4_train)

predictions2 = model2.predict(x4_test_tfidf)

from sklearn.metrics import accuracy_score
print(accuracy_score(y4_test,predictions2))

from sklearn.linear_model import LogisticRegression
classifier24 = LogisticRegression(random_state=0)
classifier24.fit(X4_train_tfidf,y4_train)

y_pred24 = classifier24.predict(x4_test_tfidf)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

# Function to calculate accuracy
def cal_accuracy(y4_test, y_pred24):
     
    print("Confusion Matrix: ",
        confusion_matrix(y4_test, y_pred24))
     
    print ("Accuracy : ",
    accuracy_score(y4_test,y_pred24)*100)
     
    print("Report : ",
    classification_report(y4_test, y_pred24))

cal_accuracy(y4_test,y_pred24)

from sklearn.neighbors import KNeighborsClassifier
classifier12 = KNeighborsClassifier()
classifier12 = classifier12.fit(X4_train_tfidf,y4_train)

y_pred12 = classifier12.predict(x4_test_tfidf)

cal_accuracy(y4_test,y_pred12)

from sklearn.svm import SVC
clf2 = SVC(kernel = "linear")
clf2.fit(X4_train_tfidf,y4_train)
pred2 = clf2.predict(x4_test_tfidf)

cal_accuracy(y4_test,pred2)

from sklearn.metrics import f1_score
f1_score(y4_test, pred2, average='macro')

from sklearn.ensemble import GradientBoostingClassifier
gbm02 = GradientBoostingClassifier(random_state=10)
gbm02 = gbm02.fit(X4_train_tfidf,y4_train)
z4 = gbm02.predict(x4_test_tfidf)
cal_accuracy(y4_test,z4)

from sklearn.ensemble import BaggingClassifier
from sklearn import tree
model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))
model.fit(X4_train_tfidf,y4_train)
model.score(x4_test_tfidf,y4_test)

from statistics import mode
model2.fit(X4_train_tfidf.toarray(),y4_train)
predictions2 = model2.predict(x4_test_tfidf.toarray())

classifier24.fit(X4_train_tfidf.toarray(),y4_train)
y_pred24 = classifier24.predict(x4_test_tfidf.toarray())

clf2.fit(X4_train_tfidf.toarray(),y4_train)
pred2 = clf2.predict(x4_test_tfidf.toarray())

final_pred = np.array([])
for i in range(0,len(x4_test_tfidf.toarray())):
    final_pred = np.append(final_pred, mode([predictions2[i], y_pred24[i], y_pred12[i], pred2[i]]))